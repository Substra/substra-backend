{{- if .Values.prePulledImages }}
{{- if eq .Values.backend.imageBuilder.type "kaniko" }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ template "substra.fullname" . }}-kaniko-cache-warmer
  labels:
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}
    app.kubernetes.io/name: {{ template "substra.name" . }}-kaniko-cache-warmer
    app.kubernetes.io/part-of: {{ template "substra.name" . }}
spec:
  template:
    spec:
      # Run the cache warmer on the same node as the worker pod, because
      # they both use the "docker-cache" PV.
      # Use "server" pod affinity (and not "worker" pod) because the server
      # is started earlier, so the warmer can also start earlier. The worker
      # is always scheduled on the same node as the server.
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - {{ template "substra.name" . }}-server
            topologyKey: kubernetes.io/hostname
      containers:
      - name: kaniko-cache-warmer
        image: gcr.io/kaniko-project/warmer:latest
        args:
        - "--cache-dir=/cache"
        {{- range .Values.prePulledImages }}
        - "--image={{ . }}"
        {{- end }}
        volumeMounts:
        - name: kaniko-cache
          mountPath: /cache
          readOnly: False
      restartPolicy: Never
      volumes:
      - name: kaniko-cache
        persistentVolumeClaim:
          claimName: {{ include "substra.fullname" $ }}-docker-cache
  backoffLimit: 4

{{- end}}

{{- if eq .Values.backend.imageBuilder.type "dind" }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ template "substra.fullname" . }}-dind-cache-warmer
  labels:
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}
    app.kubernetes.io/name: {{ template "substra.name" . }}-dind-cache-warmer
    app.kubernetes.io/part-of: {{ template "substra.name" . }}
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
        app.kubernetes.io/name: {{ template "substra.name" $ }}-dind-cache-warmer
        app.kubernetes.io/instance: {{ $.Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ template "substra.name" $ }}-dind-cache-warmer
        app.kubernetes.io/instance: {{ $.Release.Name }}
    spec:
      # Run the cache warmer on the same node as the worker pod, because
      # they both use the "docker-cache" PV.
      # Use "server" pod affinity (and not "worker" pod) because the server
      # is started earlier, so the warmer can also start earlier. The worker
      # is always scheduled on the same node as the server.
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - {{ template "substra.name" . }}-server
            topologyKey: kubernetes.io/hostname
      initContainers:
      - name: populate-warmup-folder
        image: docker:19.03-dind
        command: ['/bin/sh', '-c']
        args:
        - "dockerd-entrypoint.sh & while ! (docker ps); do sleep 1; done; docker pull {{ join "; docker pull " .Values.prePulledImages }}"
        volumeMounts:
        - name: dind-cache
          mountPath: /var/lib/docker
          subPath: warm
          readOnly: False
        securityContext:
            privileged: true
      - name: copy-to-cache
        image: alpine:latest
        command: ['sh', '-c']
        args:
        - |
          if [ ${CONCURRENCY} -eq 1 ]; then
            mv ${WARM_PATH}/* ${CACHE_PATH}/
          else
            for i in `seq ${CONCURRENCY}`; do
              cp -R ${WARM_PATH} ${CACHE_PATH}/$i;
            done
          fi

          rm -rf ${WARM_PATH}
          find ${CACHE_PATH} -maxdepth 2
        env:
          - name: CONCURRENCY
            value: {{ .Values.celeryworker.concurrency | quote }}
          - name: CACHE_PATH
            value: /var/lib/docker
          - name: WARM_PATH
            value: /var/lib/docker/warm
        volumeMounts:
        - name: dind-cache
          mountPath: /var/lib/docker
          readOnly: False
      containers:
      - name: success
        image: alpine:latest
        command: ['sh', '-c', 'sleep infinity']
      volumes:
      - name: dind-cache
        persistentVolumeClaim:
          claimName: {{ include "substra.fullname" $ }}-docker-cache
{{- end }}

{{- if eq .Values.backend.imageBuilder.type "makisu" }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ template "substra.fullname" . }}-makisu-cache-warmer
  labels:
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}
    app.kubernetes.io/name: {{ template "substra.name" . }}-makisu-cache-warmer
    app.kubernetes.io/part-of: {{ template "substra.name" . }}
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
        app.kubernetes.io/name: {{ template "substra.name" . }}-makisu-cache-warmer
        app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ template "substra.name" . }}-makisu-cache-warmer
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      # Run the cache warmer on the same node as the worker pod, because
      # they both use the "docker-cache" PV.
      # Use "server" pod affinity (and not "worker" pod) because the server
      # is started earlier, so the warmer can also start earlier. The worker
      # is always scheduled on the same node as the server.
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - {{ template "substra.name" . }}-server
            topologyKey: kubernetes.io/hostname
      initContainers:
      {{ range $index, $img := .Values.prePulledImages }}
      - name: populate-warmup-folder-{{ $index }}
        image: gcr.io/uber-container-tools/makisu-alpine:v0.2.0
        command: ['sh', '-c']
        args:
        # with modifyfs=true, we can only launch build one time
        - "/makisu-internal/makisu build --modifyfs=true -t={{ $img }}:warm /makisu-context/"
        volumeMounts:
        - name: makisu-cache
          mountPath: /makisu-storage
          subPath: warm
          readOnly: False
        - name: dockerfile-{{ $index }}
          mountPath: /makisu-context/
          readOnly: true
      {{- end }}
      - name: copy-to-cache
        image: alpine:latest
        command: ['sh', '-c']
        args:
        - |
          if [ ${CONCURRENCY} -eq 1 ]; then
            mv ${WARM_PATH}/* ${CACHE_PATH}/
          else
            for i in `seq ${CONCURRENCY}`; do
              cp -R ${WARM_PATH} ${CACHE_PATH}/$i;
            done
          fi

          rm -rf ${WARM_PATH}
          find ${CACHE_PATH} -maxdepth 2
        env:
          - name: CONCURRENCY
            value: {{ .Values.celeryworker.concurrency | quote }}
          - name: CACHE_PATH
            value: /makisu-storage
          - name: WARM_PATH
            value: /makisu-storage/warm
        volumeMounts:
        - name: makisu-cache
          mountPath: /makisu-storage
          readOnly: False
      containers:
      - name: success
        image: alpine:latest
        command: ['sh', '-c', 'sleep infinity']
      volumes:
      - name: makisu-cache
        persistentVolumeClaim:
          claimName: {{ include "substra.fullname" . }}-docker-cache
      {{ range $index, $img := .Values.prePulledImages }}
      - name: dockerfile-{{ $index }}
        configMap:
          name: {{ template "substra.fullname" $ }}-makisu-dockerfile-{{ $index }}
      {{- end }}

{{ range $index, $img := .Values.prePulledImages }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "substra.fullname" $ }}-makisu-dockerfile-{{ $index }}
  labels:
    app.kubernetes.io/managed-by: {{ $.Release.Service }}
    app.kubernetes.io/instance: {{ $.Release.Name }}
    helm.sh/chart: {{ $.Chart.Name }}-{{ $.Chart.Version }}
    app.kubernetes.io/name: {{ template "substra.name" $ }}
data:
  Dockerfile: |
    FROM {{ $img }}
---
{{- end }}
{{- end }}

{{- end }}
