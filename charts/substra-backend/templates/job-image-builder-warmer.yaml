{{- if .Values.backend.kaniko.cache.warmer.images }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ template "substra.fullname" . }}-kaniko-cache-warmer
  labels:
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}
    app.kubernetes.io/name: {{ template "substra.name" . }}-kaniko-cache-warmer
    app.kubernetes.io/part-of: {{ template "substra.name" . }}
spec:
  template:
    spec:
      # Run the cache warmer on the same node as the worker pod, because
      # they both use the "docker-cache" PV.
      # Use "server" pod affinity (and not "worker" pod) because the server
      # is started earlier, so the warmer can also start earlier. The worker
      # is always scheduled on the same node as the server.
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - {{ template "substra.name" . }}-server
            topologyKey: kubernetes.io/hostname
      containers:
      - name: kaniko-cache-warmer
        image: {{ .Values.backend.kaniko.cache.warmer.image }}
        args:
        - "--cache-dir=/cache"
        {{- range .Values.backend.kaniko.cache.warmer.images }}
        - "--image={{ .image }}"
        {{- end }}
        - "--verbosity=debug"
        volumeMounts:
        - name: kaniko-cache
          mountPath: /cache
          readOnly: False
      restartPolicy: Never
      volumes:
      - name: kaniko-cache
        persistentVolumeClaim:
          claimName: {{ include "substra.fullname" $ }}-docker-cache
  backoffLimit: 4

{{- end}}
